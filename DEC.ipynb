{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T10:16:15.389958Z",
     "start_time": "2025-04-29T10:16:11.329921Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:18:05.772364Z",
     "start_time": "2025-04-29T10:18:02.974371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка данных\n",
    "train = load_npz(\"data/train.npz\")\n",
    "X = train.toarray()\n",
    "\n",
    "# Стандартизация\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Тензоры\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_tensor)\n",
    "loader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ],
   "id": "ccb7f083938a2ecc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:18:25.667604Z",
     "start_time": "2025-04-29T10:18:25.635603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return z, x_recon"
   ],
   "id": "625ee8e9fb182956",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:23:44.623745Z",
     "start_time": "2025-04-29T10:18:32.651937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = X_scaled.shape[1]\n",
    "latent_dim = 64\n",
    "\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder.to(device)\n",
    "\n",
    "# Предобучение\n",
    "for epoch in range(50):\n",
    "    for batch in loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        _, reconstructions = autoencoder(inputs)\n",
    "        loss = criterion(reconstructions, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Pretraining Epoch [{epoch+1}/50], Loss: {loss.item():.4f}\")"
   ],
   "id": "3845e510382cd25b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Epoch [1/50], Loss: 0.7369\n",
      "Pretraining Epoch [2/50], Loss: 0.7522\n",
      "Pretraining Epoch [3/50], Loss: 0.8757\n",
      "Pretraining Epoch [4/50], Loss: 0.7821\n",
      "Pretraining Epoch [5/50], Loss: 0.7413\n",
      "Pretraining Epoch [6/50], Loss: 0.7602\n",
      "Pretraining Epoch [7/50], Loss: 0.7709\n",
      "Pretraining Epoch [8/50], Loss: 0.7339\n",
      "Pretraining Epoch [9/50], Loss: 0.7373\n",
      "Pretraining Epoch [10/50], Loss: 0.7200\n",
      "Pretraining Epoch [11/50], Loss: 0.7338\n",
      "Pretraining Epoch [12/50], Loss: 0.7159\n",
      "Pretraining Epoch [13/50], Loss: 0.7089\n",
      "Pretraining Epoch [14/50], Loss: 0.7864\n",
      "Pretraining Epoch [15/50], Loss: 0.7694\n",
      "Pretraining Epoch [16/50], Loss: 0.7515\n",
      "Pretraining Epoch [17/50], Loss: 0.7566\n",
      "Pretraining Epoch [18/50], Loss: 0.7647\n",
      "Pretraining Epoch [19/50], Loss: 0.7474\n",
      "Pretraining Epoch [20/50], Loss: 0.6993\n",
      "Pretraining Epoch [21/50], Loss: 0.6934\n",
      "Pretraining Epoch [22/50], Loss: 0.7363\n",
      "Pretraining Epoch [23/50], Loss: 0.7144\n",
      "Pretraining Epoch [24/50], Loss: 0.7430\n",
      "Pretraining Epoch [25/50], Loss: 0.7318\n",
      "Pretraining Epoch [26/50], Loss: 0.7480\n",
      "Pretraining Epoch [27/50], Loss: 0.6856\n",
      "Pretraining Epoch [28/50], Loss: 0.7225\n",
      "Pretraining Epoch [29/50], Loss: 0.7138\n",
      "Pretraining Epoch [30/50], Loss: 0.7260\n",
      "Pretraining Epoch [31/50], Loss: 0.7028\n",
      "Pretraining Epoch [32/50], Loss: 0.7160\n",
      "Pretraining Epoch [33/50], Loss: 0.7262\n",
      "Pretraining Epoch [34/50], Loss: 0.7112\n",
      "Pretraining Epoch [35/50], Loss: 0.7464\n",
      "Pretraining Epoch [36/50], Loss: 0.6979\n",
      "Pretraining Epoch [37/50], Loss: 0.7097\n",
      "Pretraining Epoch [38/50], Loss: 0.6963\n",
      "Pretraining Epoch [39/50], Loss: 0.7508\n",
      "Pretraining Epoch [40/50], Loss: 0.6799\n",
      "Pretraining Epoch [41/50], Loss: 0.7154\n",
      "Pretraining Epoch [42/50], Loss: 0.7171\n",
      "Pretraining Epoch [43/50], Loss: 0.6973\n",
      "Pretraining Epoch [44/50], Loss: 0.6979\n",
      "Pretraining Epoch [45/50], Loss: 0.6938\n",
      "Pretraining Epoch [46/50], Loss: 0.6862\n",
      "Pretraining Epoch [47/50], Loss: 0.7250\n",
      "Pretraining Epoch [48/50], Loss: 0.7040\n",
      "Pretraining Epoch [49/50], Loss: 0.7348\n",
      "Pretraining Epoch [50/50], Loss: 0.7604\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:24:17.019683Z",
     "start_time": "2025-04-29T10:24:15.086695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    latent = autoencoder.encoder(X_tensor).numpy()\n",
    "\n",
    "# K-Means\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_centers = kmeans.fit(latent).cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Сохраняем центры как параметр модели\n",
    "cluster_centers_torch = torch.tensor(cluster_centers, dtype=torch.float32, device=device)"
   ],
   "id": "211522009a0e67a4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:24:21.859081Z",
     "start_time": "2025-04-29T10:24:21.848080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()"
   ],
   "id": "5a7a3d2ef17655a5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:25:45.570177Z",
     "start_time": "2025-04-29T10:25:07.251715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F  # <- добавляем F\n",
    "\n",
    "# DEC Training\n",
    "for epoch in range(10):\n",
    "    for batch in loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        z, _ = autoencoder(inputs)\n",
    "\n",
    "        # Вычисляем мягкое принадлежность к кластерам\n",
    "        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - cluster_centers_torch) ** 2, dim=2) / 1)\n",
    "        q = q.pow(2)\n",
    "        q = q / q.sum(1, keepdim=True)\n",
    "\n",
    "        p = target_distribution(q)\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        loss = F.kl_div(q.log(), p, reduction='batchmean')  # <- ИСПРАВЛЕНО!\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"DEC Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")"
   ],
   "id": "b241da3267b198f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEC Epoch [1/10], Loss: 0.0008\n",
      "DEC Epoch [2/10], Loss: 0.0003\n",
      "DEC Epoch [3/10], Loss: 0.0002\n",
      "DEC Epoch [4/10], Loss: 0.0003\n",
      "DEC Epoch [5/10], Loss: 0.0001\n",
      "DEC Epoch [6/10], Loss: 0.0001\n",
      "DEC Epoch [7/10], Loss: 0.0001\n",
      "DEC Epoch [8/10], Loss: 0.0001\n",
      "DEC Epoch [9/10], Loss: 0.0001\n",
      "DEC Epoch [10/10], Loss: 0.0001\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:26:04.542812Z",
     "start_time": "2025-04-29T10:26:03.218814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    z_final = autoencoder.encoder(X_tensor).numpy()\n",
    "\n",
    "labels_final = kmeans.fit_predict(z_final)"
   ],
   "id": "aaf52c2ac0ba0fc9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:26:19.396061Z",
     "start_time": "2025-04-29T10:26:10.721069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score(z_final, labels_final):.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz_score(z_final, labels_final):.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin_score(z_final, labels_final):.4f}\")"
   ],
   "id": "68297b2cbf45dd7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.1355\n",
      "Calinski-Harabasz Score: 3992.52\n",
      "Davies-Bouldin Score: 2.0311\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:29:42.355917Z",
     "start_time": "2025-04-29T10:29:42.315927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": range(len(labels_final)),\n",
    "    \"TARGET\": labels_final\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_submit_DEC.csv\", index=False)"
   ],
   "id": "87893a6580d21e10",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
